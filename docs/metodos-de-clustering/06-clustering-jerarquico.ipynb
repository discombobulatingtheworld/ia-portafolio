{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Jerárquico\n",
    "\n",
    "El clustering jerárquico es un enfoque alternativo al clustering que no requiere especificar previamente el número de grupos (K) y resulta en una representación basada en árboles llamada dendrograma. En este contexto, se describe la agrupación ascendente o aglomerativa, que es el tipo más común de clustering jerárquico.\n",
    "\n",
    "## Interpretación de un Dendrograma\n",
    "\n",
    "El dendrograma representa la relación entre observaciones en un conjunto de datos. Cada hoja del dendrograma corresponde a una observación en los datos. A medida que subimos en el árbol, las hojas se fusionan en ramas, indicando que las observaciones son similares entre sí. Las fusiones tempranas indican similitud, mientras que las fusiones posteriores indican diferencias. La altura de la fusión en el eje vertical indica cuán diferentes son dos observaciones. Observaciones que se fusionan en la parte inferior del árbol son similares, mientras que las que se fusionan cerca de la parte superior son diferentes. No se debe concluir que dos observaciones son similares solo porque estén cerca en el dendrograma. La similitud se basa en la altura de fusión en el eje vertical.\n",
    "\n",
    "Cortar el dendrograma en una altura específica produce grupos. La elección de la altura de corte controla el número de grupos obtenidos. Uno puede seleccionar visualmente un número sensato de grupos o usar métodos estadísticos para determinarlo. El dendrograma permite obtener cualquier número de grupos, desde 1 hasta n (el número de observaciones), haciendo cortes a diferentes alturas. La estructura jerárquica implica que los grupos obtenidos a alturas menores están anidados dentro de los grupos obtenidos a alturas mayores. Sin embargo, en algunos casos, la suposición de estructura jerárquica puede ser irrealista, y el clustering jerárquico puede ser menos preciso que el clustering K-means para ciertos números de grupos.\n",
    "\n",
    "## Algoritmo de Clustering Jerárquico\n",
    "\n",
    "Comenzamos definiendo alguna medida de disimilitud entre cada par de observaciones. Con mayor frecuencia, se utiliza la distancia euclidiana. El algoritmo procede de manera iterativa. Comenzando en la parte inferior del dendrograma, cada una de las n observaciones se trata como su propio grupo. Luego, se fusionan los dos grupos que son más similares entre sí, de modo que ahora hay n-1 grupos. A continuación, se fusionan nuevamente los dos grupos que son más similares entre sí, de modo que ahora hay n-2 grupos. El algoritmo continúa de esta manera hasta que todas las observaciones pertenecen a un solo grupo y el dendrograma está completo.\n",
    "\n",
    "## Elección de la Medida de Disimilitud\n",
    "\n",
    "Uno de los desafíos del clustering jerárquico es definir la disimilitud entre dos grupos de observaciones. Esto se logra a través del concepto de enlace, que define cómo se calcula la disimilitud entre grupos. Los cuatro tipos más comunes de enlace son:\n",
    "\n",
    "- **Completo:** Define la disimilitud entre dos grupos como la máxima disimilitud entre todas las combinaciones de observaciones en ambos grupos.\n",
    "- **Promedio:** Calcula la disimilitud entre grupos como el promedio de todas las disimilitudes entre las observaciones de los dos grupos.\n",
    "- **Simple:** Define la disimilitud entre grupos como la mínima disimilitud entre todas las combinaciones de observaciones en ambos grupos.\n",
    "- **Centroide:** Utiliza el centroide (el punto medio) de cada grupo para calcular la disimilitud entre grupos.\n",
    "  \n",
    "La elección del tipo de enlace afecta significativamente la estructura del dendrograma resultante y, por lo tanto, debe seleccionarse cuidadosamente según la aplicación y los datos específicos.\n",
    "\n",
    "La elección de la medida de disimilitud es crucial y debe basarse en el tipo de datos y la pregunta científica. La distancia euclidiana es común, pero otras medidas como la distancia basada en correlación pueden ser preferibles en ciertos casos. La distancia basada en correlación se centra en las relaciones entre las observaciones, mientras que la distancia euclidiana se centra en las diferencias absolutas entre las observaciones. Además, se debe considerar si las variables deben escalarse antes de calcular la disimilitud, lo que puede influir en el impacto relativo de las variables en el clustering jerárquico."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
