{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering K-Means\n",
    "\n",
    "Clustering K-Means es un algoritmo de agrupación ampliamente utilizado que busca dividir un conjunto de datos en k clústeres, donde cada clúster contiene puntos de datos similares entre sí y distintos de los puntos en otros clústeres. Aquí se detallan los conceptos clave de la agrupación K-Means:\n",
    "\n",
    "## Proceso de K-Means\n",
    "\n",
    "La lógica de encontrar k-clústeres con un conjunto de datos dado es bastante simple y siempre converge hacia una solución. Sin embargo, en la mayoría de los casos, el resultado final será localmente óptimo, y la solución no convergerá hacia la mejor solución global.\n",
    "\n",
    "1. **Inicialización de centroides:** El proceso comienza con la selección inicial de k centroides (puntos representativos) en el espacio de datos. El usuario debe especificar el número de clústeres, k.\n",
    "\n",
    "2. **Asignación de puntos:** Luego, todos los puntos de datos se asignan al centroide más cercano, utilizando una medida de proximidad, típicamente la distancia euclidiana.\n",
    "   \n",
    "$$\n",
    "\\text{Distance } d = \\sqrt{(x_1 - c_1)^2 + (x_2 - c_2)^2 + \\ldots + (x_n - c_n)^2}\n",
    "$$\n",
    "\n",
    "1. **Cálculo de nuevos centroides:** Para cada clúster, se calcula un nuevo centroide, que es el punto de datos más representativo del clúster. Esto se hace minimizando la suma de errores al cuadrado (SSE), que mide la diferencia entre los puntos de datos y el centroide del clúster.\n",
    "   \n",
    "$$\n",
    "SSE = \\sum_{i=1}^k \\sum_{x \\in C_i} ||x_j - \\mu_i||^2\n",
    "$$\n",
    "    \n",
    "$$\n",
    "\\mu_i = \\frac{1}{j_i} \\sum_{x \\in c_i} X\n",
    "$$\n",
    "\n",
    "4. **Repetición de asignación y cálculo:** Se repiten los pasos 2 y 3 hasta que no haya un cambio significativo en la asignación de puntos o en la ubicación de los centroides.\n",
    "   \n",
    "5. **Terminación:** Cuando se alcanza la convergencia, los centroides finales representan los prototipos de los clústeres y se utilizan para describir el modelo de agrupación. Cada punto de datos se asocia con un atributo de identificación de agrupación.\n",
    "\n",
    "## Consideraciones Especiales\n",
    "\n",
    "- **Inicialización:** La calidad de la solución depende de la inicialización de los centroides. Múltiples inicializaciones aleatorias y la elección de la ejecución con la SSE total más baja pueden mejorar los resultados.\n",
    "- **Clústeres Vacíos:** Pueden formarse clústeres vacíos, lo que se soluciona introduciendo un nuevo centroide en el clúster que contribuye a la SSE más alta.\n",
    "- **Valores Atípicos:** Los valores atípicos pueden afectar negativamente el resultado al alejar el centroide de los datos representativos. Las técnicas de preprocesamiento o identificación de valores atípicos pueden abordar este problema.\n",
    "- **Postprocesamiento:** A veces, es útil aplicar técnicas de postprocesamiento, como ajustar el número de clústeres o fusionar clústeres para mejorar la solución.\n",
    "  \n",
    "K-Means es eficiente y simple de implementar, pero busca una solución óptima local, lo que significa que depende de la inicialización de centroides y puede no encontrar la mejor solución global. Por tanto, es importante entender sus limitaciones y aplicar estrategias para superarlas.\n",
    "\n",
    "## Evaluación de los clústeres\n",
    "\n",
    "La evaluación de clústeres en el contexto de la agrupación K-Means es crucial para determinar la calidad de la agrupación. Dado que no hay etiquetas externas conocidas para la comparación en la agrupación, se recurre a la evaluación interna. Aquí se describen las métricas de evaluación clave:\n",
    "\n",
    "1. **Suma de Errores al Cuadrado (SSE):** La SSE es una métrica simple que mide la calidad de la agrupación en términos de distancia promedio dentro de cada clúster. Los buenos modelos de agrupación tienen una SSE baja, lo que significa que los puntos de datos dentro de un clúster están cerca unos de otros. Además, una SSE baja entre todos los clústeres indica una buena separación entre los clústeres.\n",
    "   \n",
    "2. **Índice Davies-Bouldin:** Este índice se basa en la singularidad de los clústeres y considera tanto la cohesión dentro del clúster (la distancia entre los puntos de datos y el centro del clúster) como la separación entre los clústeres. Cuanto menor sea el valor del índice Davies-Bouldin, mejor será la agrupación. En otras palabras, busca clústeres más compactos y bien separados. Esta métrica es una medida más completa que la SSE, ya que considera tanto la distancia intraclúster como la distancia interclúster. Sin embargo, al igual que la SSE, no garantiza una mejor agrupación cuando tiene valores más bajos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
